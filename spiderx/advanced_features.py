"""
Advanced Features Module
ÙˆØ­Ø¯Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© Ø§Ù„Ø£Ø³Ø·ÙˆØ±ÙŠØ©

Legendary competitive features that surpass ParamSpider
"""

import asyncio
import json
import re
from typing import Dict, List, Set, Tuple, Optional
from collections import defaultdict, Counter
from urllib.parse import urlparse, parse_qs
from datetime import datetime

from .utils import print_info, print_success, print_error, print_warning


class ParameterIntelligence:
    """AI-powered parameter analysis - LEGENDARY FEATURE"""
    
    def __init__(self):
        # Parameter categories with intelligence
        self.param_categories = {
            'authentication': ['auth', 'token', 'key', 'password', 'pass', 'login', 'user', 'username'],
            'database': ['id', 'uid', 'product_id', 'user_id', 'item_id', 'order_id', 'post_id'],
            'search': ['search', 'query', 'q', 'keyword', 'term', 'find', 'lookup'],
            'navigation': ['page', 'limit', 'offset', 'sort', 'order', 'direction'],
            'filters': ['category', 'type', 'filter', 'status', 'state', 'tag'],
            'debug': ['debug', 'test', 'dev', 'trace', 'verbose', 'log', 'error'],
            'file_operations': ['file', 'path', 'filename', 'upload', 'download', 'attachment'],
            'redirects': ['redirect', 'url', 'next', 'return', 'continue', 'goto']
        }
        
        # Risk scoring patterns
        self.risk_patterns = {
            'critical': ['password', 'secret', 'key', 'token', 'auth'],
            'high': ['admin', 'user', 'id', 'search', 'query', 'file', 'path'],
            'medium': ['page', 'sort', 'filter', 'category', 'type'],
            'low': ['lang', 'theme', 'view', 'format']
        }
    
    def analyze_parameter_intelligence(self, params: Set[str]) -> Dict:
        """Intelligent parameter analysis"""
        analysis = {
            'categories': defaultdict(list),
            'risk_levels': defaultdict(list),
            'recommendations': [],
            'interesting_patterns': []
        }
        
        for param in params:
            # Categorize parameter
            category = self._categorize_parameter(param)
            if category:
                analysis['categories'][category].append(param)
            
            # Assess risk level
            risk = self._assess_risk_level(param)
            analysis['risk_levels'][risk].append(param)
            
            # Find interesting patterns
            patterns = self._find_interesting_patterns(param)
            analysis['interesting_patterns'].extend(patterns)
        
        # Generate recommendations
        analysis['recommendations'] = self._generate_recommendations(analysis)
        
        return dict(analysis)
    
    def _categorize_parameter(self, param: str) -> Optional[str]:
        """Categorize parameter based on intelligence patterns"""
        param_lower = param.lower()
        
        for category, patterns in self.param_categories.items():
            if any(pattern in param_lower for pattern in patterns):
                return category
        
        return 'unknown'
    
    def _assess_risk_level(self, param: str) -> str:
        """Assess security risk level of parameter"""
        param_lower = param.lower()
        
        for risk_level, patterns in self.risk_patterns.items():
            if any(pattern in param_lower for pattern in patterns):
                return risk_level
        
        return 'low'
    
    def _find_interesting_patterns(self, param: str) -> List[str]:
        """Find interesting patterns in parameter names"""
        patterns = []
        
        # Check for common vulnerability indicators
        if re.search(r'(cmd|exec|shell|system)', param, re.IGNORECASE):
            patterns.append(f"Command injection potential: {param}")
        
        if re.search(r'(file|path|include|template)', param, re.IGNORECASE):
            patterns.append(f"File inclusion potential: {param}")
        
        if re.search(r'(redirect|url|next|goto)', param, re.IGNORECASE):
            patterns.append(f"Open redirect potential: {param}")
        
        if re.search(r'(debug|test|dev|admin)', param, re.IGNORECASE):
            patterns.append(f"Debug/Admin parameter: {param}")
        
        return patterns
    
    def _generate_recommendations(self, analysis: Dict) -> List[str]:
        """Generate testing recommendations"""
        recommendations = []
        
        # High-risk parameters
        high_risk = analysis['risk_levels'].get('high', [])
        if high_risk:
            recommendations.append(f"Prioritize testing these high-risk parameters: {', '.join(high_risk[:5])}")
        
        # Authentication parameters
        auth_params = analysis['categories'].get('authentication', [])
        if auth_params:
            recommendations.append(f"Test authentication bypass with: {', '.join(auth_params)}")
        
        # Database parameters
        db_params = analysis['categories'].get('database', [])
        if db_params:
            recommendations.append(f"Test SQL injection with: {', '.join(db_params)}")
        
        # Search parameters
        search_params = analysis['categories'].get('search', [])
        if search_params:
            recommendations.append(f"Test XSS with: {', '.join(search_params)}")
        
        return recommendations


class AdvancedFiltering:
    """Advanced filtering with ML-like capabilities - LEGENDARY FEATURE"""
    
    def __init__(self):
        # Advanced boring patterns with confidence scores
        self.advanced_boring_patterns = {
            # Tracking patterns (95% confidence)
            r'^utm_.*': 0.95,
            r'^ga_.*': 0.95,
            r'^_ga.*': 0.95,
            r'.*tracking.*': 0.90,
            r'.*analytics.*': 0.90,
            
            # Session patterns (90% confidence)
            r'^session.*': 0.90,
            r'.*sess.*': 0.85,
            r'^sid$': 0.90,
            
            # Cache patterns (85% confidence)
            r'^cache.*': 0.85,
            r'^_+$': 0.90,  # Underscore cache busters
            r'^ts$|^timestamp$': 0.85,
            r'^v$|^version$': 0.80,
            
            # Social media (80% confidence)
            r'^ref$|^referrer$': 0.80,
            r'^fbclid$|^gclid$': 0.95,
            r'^share.*': 0.75,
        }
        
        # Interesting parameter patterns (keep these)
        self.interesting_patterns = {
            r'.*search.*': 0.95,
            r'.*query.*': 0.95,
            r'.*admin.*': 0.90,
            r'.*user.*': 0.85,
            r'.*id$': 0.80,
            r'.*key.*': 0.90,
            r'.*token.*': 0.90,
            r'.*auth.*': 0.90,
            r'.*file.*': 0.85,
            r'.*path.*': 0.85,
        }
    
    def smart_filter_parameters(self, params: Set[str], threshold: float = 0.7) -> Tuple[Set[str], Dict]:
        """Smart filtering with confidence scoring"""
        filtered_params = set()
        filtering_stats = {
            'total_params': len(params),
            'filtered_out': 0,
            'kept_params': 0,
            'confidence_scores': {},
            'filter_reasons': defaultdict(list)
        }
        
        for param in params:
            # Calculate boring score
            boring_score = self._calculate_boring_score(param)
            
            # Calculate interesting score
            interesting_score = self._calculate_interesting_score(param)
            
            # Final decision based on scores
            final_score = interesting_score - boring_score
            filtering_stats['confidence_scores'][param] = final_score
            
            if final_score >= threshold:
                filtered_params.add(param)
                filtering_stats['kept_params'] += 1
            else:
                filtering_stats['filtered_out'] += 1
                self._record_filter_reason(param, boring_score, filtering_stats)
        
        return filtered_params, filtering_stats
    
    def _calculate_boring_score(self, param: str) -> float:
        """Calculate how boring a parameter is"""
        max_score = 0.0
        
        for pattern, confidence in self.advanced_boring_patterns.items():
            if re.match(pattern, param, re.IGNORECASE):
                max_score = max(max_score, confidence)
        
        return max_score
    
    def _calculate_interesting_score(self, param: str) -> float:
        """Calculate how interesting a parameter is"""
        max_score = 0.0
        
        for pattern, confidence in self.interesting_patterns.items():
            if re.match(pattern, param, re.IGNORECASE):
                max_score = max(max_score, confidence)
        
        # Boost score for complex parameters
        if len(param) > 6:
            max_score += 0.1
        if '_' in param or '-' in param:
            max_score += 0.05
        
        return max_score
    
    def _record_filter_reason(self, param: str, boring_score: float, stats: Dict):
        """Record why parameter was filtered"""
        if boring_score > 0.9:
            stats['filter_reasons']['high_confidence_boring'].append(param)
        elif boring_score > 0.7:
            stats['filter_reasons']['medium_confidence_boring'].append(param)
        else:
            stats['filter_reasons']['low_interesting_score'].append(param)


class RealTimeAnalyzer:
    """Real-time analysis and reporting - LEGENDARY FEATURE"""
    
    def __init__(self):
        self.start_time = datetime.now()
        self.stats = {
            'domains_processed': 0,
            'urls_discovered': 0,
            'parameters_found': 0,
            'sources_used': set(),
            'error_count': 0,
            'processing_times': []
        }
    
    def update_stats(self, domain: str, urls: List[str], source: str):
        """Update real-time statistics"""
        self.stats['domains_processed'] += 1
        self.stats['urls_discovered'] += len(urls)
        self.stats['sources_used'].add(source)
        
        # Count unique parameters
        params = set()
        for url in urls:
            try:
                parsed = urlparse(url)
                url_params = parse_qs(parsed.query)
                params.update(url_params.keys())
            except Exception:
                continue
        
        self.stats['parameters_found'] += len(params)
    
    def record_error(self, error_type: str, details: str):
        """Record processing errors"""
        self.stats['error_count'] += 1
        
        if not hasattr(self, 'errors'):
            self.errors = []
        
        self.errors.append({
            'type': error_type,
            'details': details,
            'timestamp': datetime.now()
        })
    
    def get_performance_metrics(self) -> Dict:
        """Get comprehensive performance metrics"""
        elapsed_time = (datetime.now() - self.start_time).total_seconds()
        
        metrics = {
            'execution_time': elapsed_time,
            'domains_per_second': self.stats['domains_processed'] / elapsed_time if elapsed_time > 0 else 0,
            'urls_per_second': self.stats['urls_discovered'] / elapsed_time if elapsed_time > 0 else 0,
            'success_rate': (1 - (self.stats['error_count'] / max(1, self.stats['domains_processed']))) * 100,
            'average_params_per_domain': self.stats['parameters_found'] / max(1, self.stats['domains_processed']),
            'sources_coverage': len(self.stats['sources_used'])
        }
        
        return metrics
    
    def generate_live_report(self) -> str:
        """Generate live progress report"""
        metrics = self.get_performance_metrics()
        
        report = []
        report.append("âš¡ SPIDERX LIVE PERFORMANCE REPORT")
        report.append("=" * 40)
        report.append(f"ğŸ• Runtime: {metrics['execution_time']:.1f}s")
        report.append(f"ğŸŒ Domains: {self.stats['domains_processed']}")
        report.append(f"ğŸ”— URLs Found: {self.stats['urls_discovered']}")
        report.append(f"ğŸ¯ Parameters: {self.stats['parameters_found']}")
        report.append(f"ğŸ“Š Success Rate: {metrics['success_rate']:.1f}%")
        report.append(f"âš¡ Speed: {metrics['urls_per_second']:.1f} URLs/sec")
        report.append(f"ğŸ“¡ Sources: {', '.join(self.stats['sources_used'])}")
        
        return "\n".join(report)


class CompetitiveEdge:
    """Features that make SpiderX legendary compared to ParamSpider"""
    
    @staticmethod
    def get_competitive_advantages() -> Dict[str, str]:
        """Get all competitive advantages over ParamSpider"""
        return {
            "Multiple Data Sources": "Wayback + Sitemap + JavaScript + Live Crawling vs ParamSpider's Wayback only",
            "Smart Filtering": "60+ boring parameters + wildcards + ML-like confidence scoring vs basic hardcoded list",
            "Vulnerability Detection": "Real-time vulnerability parameter detection with risk scoring",
            "Export Formats": "TXT + CSV + JSON with metadata vs TXT only",
            "Performance": "Async multi-threaded processing vs single-threaded",
            "Intelligence": "Parameter categorization and testing recommendations",
            "Rate Limiting": "Smart adaptive rate limiting vs none",
            "Error Handling": "Robust retry mechanism with exponential backoff",
            "Statistics": "Comprehensive analytics and live reporting",
            "Customization": "Highly configurable with 20+ CLI options vs 5 basic options",
            "User Experience": "Professional progress bars and colored output",
            "Architecture": "Modular design for easy extension vs monolithic"
        }
    
    @staticmethod
    def display_competitive_summary():
        """Display why SpiderX is superior"""
        advantages = CompetitiveEdge.get_competitive_advantages()
        
        print_success("ğŸ† SPIDERX COMPETITIVE ADVANTAGES")
        print("=" * 50)
        
        for feature, description in advantages.items():
            print_info(f"âœ“ {feature}")
            print(f"  {description}")
            print("")
        
        print_success("ğŸ•·ï¸ SpiderX: The LEGENDARY URL parameter discovery tool!")
        print_info("Built to completely surpass ParamSpider with professional features")


class LegendaryStats:
    """Comprehensive statistics that blow ParamSpider away"""
    
    def __init__(self):
        self.detailed_stats = {
            'parameter_frequency': Counter(),
            'domain_coverage': defaultdict(set),
            'source_effectiveness': defaultdict(int),
            'vulnerability_findings': defaultdict(list),
            'processing_timeline': [],
            'error_analysis': defaultdict(int)
        }
    
    def record_discovery(self, param: str, domain: str, source: str, url: str):
        """Record parameter discovery with full details"""
        self.detailed_stats['parameter_frequency'][param] += 1
        self.detailed_stats['domain_coverage'][param].add(domain)
        self.detailed_stats['source_effectiveness'][source] += 1
        
        # Record timeline
        self.detailed_stats['processing_timeline'].append({
            'timestamp': datetime.now(),
            'param': param,
            'domain': domain,
            'source': source,
            'url': url
        })
    
    def generate_legendary_report(self) -> str:
        """Generate comprehensive report that showcases SpiderX superiority"""
        report = []
        
        report.append("ğŸ•·ï¸ SPIDERX LEGENDARY ANALYSIS REPORT")
        report.append("=" * 60)
        report.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # Top parameters
        if self.detailed_stats['parameter_frequency']:
            report.append("ğŸ† TOP DISCOVERED PARAMETERS:")
            for param, count in self.detailed_stats['parameter_frequency'].most_common(15):
                domains = len(self.detailed_stats['domain_coverage'][param])
                report.append(f"  {param}: {count} occurrences across {domains} domains")
            report.append("")
        
        # Source effectiveness
        if self.detailed_stats['source_effectiveness']:
            report.append("ğŸ“Š SOURCE EFFECTIVENESS:")
            total = sum(self.detailed_stats['source_effectiveness'].values())
            for source, count in self.detailed_stats['source_effectiveness'].items():
                percentage = (count / total) * 100
                report.append(f"  {source.capitalize()}: {count} parameters ({percentage:.1f}%)")
            report.append("")
        
        # Discovery timeline
        if self.detailed_stats['processing_timeline']:
            report.append("â° DISCOVERY TIMELINE:")
            report.append(f"  First discovery: {self.detailed_stats['processing_timeline'][0]['timestamp']}")
            report.append(f"  Last discovery: {self.detailed_stats['processing_timeline'][-1]['timestamp']}")
            report.append(f"  Total discoveries: {len(self.detailed_stats['processing_timeline'])}")
            report.append("")
        
        report.append("ğŸš€ SpiderX: Legendary URL parameter discovery completed!")
        
        return "\n".join(report)