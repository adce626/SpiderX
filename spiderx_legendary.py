#!/usr/bin/env python3
"""
SpiderX CLI - LEGENDARY VERSION
Ø£Ø¯Ø§Ø© SpiderX Ø§Ù„Ø£Ø³Ø·ÙˆØ±ÙŠØ© Ø§Ù„Ù…Ø·ÙˆØ±Ø©

The ultimate URL parameter mining tool that completely surpasses ParamSpider
"""

import asyncio
import argparse
import sys
import time
from pathlib import Path
from typing import List, Dict, Set
from collections import Counter

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±
sys.path.insert(0, str(Path(__file__).parent))

from spiderx.utils import (
    print_banner, print_info, print_success, print_error, print_warning,
    get_random_user_agent, smart_rate_limit, detect_potential_vulnerability_params,
    analyze_parameter_complexity
)
from spiderx.sources.wayback import WaybackSource
from spiderx.sources.sitemap import SitemapSource  
from spiderx.sources.javascript import JavaScriptSource
from spiderx.filters import ParameterFilter
from spiderx.exporters import ResultExporter


class LegendarySpiderX:
    """SpiderX Ø§Ù„Ø£Ø³Ø·ÙˆØ±ÙŠØ© - ØªØªÙÙˆÙ‚ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…Ù†Ø§ÙØ³Ø©"""
    
    def __init__(self, args):
        self.args = args
        self.start_time = time.time()
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù…ØªÙ‚Ø¯Ù…Ø©
        self.stats = {
            'domains_processed': 0,
            'total_urls_found': 0,
            'parameters_discovered': set(),
            'vulnerability_findings': {},
            'source_effectiveness': Counter(),
            'processing_errors': []
        }
        
        # Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ·ÙˆØ±Ø©
        self.sources = {
            'wayback': WaybackSource(args),
            'sitemap': SitemapSource(args),
            'javascript': JavaScriptSource(args)
        }
        
        # ÙÙ„ØªØ± Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ø°ÙƒÙŠ
        self.filter = ParameterFilter(args)
        
        # Ù…ÙØµØ¯Ø± Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        self.exporter = ResultExporter(args)
    
    async def legendary_scan(self, domains: List[str]) -> Dict:
        """Ø§Ù„Ù…Ø³Ø­ Ø§Ù„Ø£Ø³Ø·ÙˆØ±ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„"""
        print_banner()
        print_success("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø³Ø­ Ø§Ù„Ø£Ø³Ø·ÙˆØ±ÙŠ Ø¨Ù€ SpiderX!")
        print_info(f"Ø§Ù„Ø£Ù‡Ø¯Ø§Ù: {len(domains)} Ù†Ø·Ø§Ù‚")
        print("")
        
        all_results = {
            'urls': [],
            'parameters': Counter(),
            'domains': set(domains),
            'vulnerability_analysis': {},
            'source_stats': {},
            'performance_metrics': {}
        }
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ Ù†Ø·Ø§Ù‚
        for domain in domains:
            print_info(f"ğŸ¯ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Ø·Ø§Ù‚: {domain}")
            domain_results = await self._process_domain_legendary(domain)
            
            # Ø¯Ù…Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            all_results['urls'].extend(domain_results.get('urls', []))
            all_results['parameters'].update(domain_results.get('parameters', {}))
            
            self.stats['domains_processed'] += 1
            
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù„Ù„Ø«ØºØ±Ø§Øª Ø§Ù„Ø£Ù…Ù†ÙŠØ© - Ù…ÙŠØ²Ø© Ø£Ø³Ø·ÙˆØ±ÙŠØ©
        print_info("ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù„Ù„Ø«ØºØ±Ø§Øª Ø§Ù„Ø£Ù…Ù†ÙŠØ©...")
        vulnerability_analysis = self._analyze_vulnerabilities(all_results['parameters'])
        all_results['vulnerability_analysis'] = vulnerability_analysis
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
        execution_time = time.time() - self.start_time
        all_results['performance_metrics'] = {
            'execution_time': execution_time,
            'domains_processed': self.stats['domains_processed'],
            'total_parameters': len(all_results['parameters']),
            'urls_per_second': len(all_results['urls']) / execution_time if execution_time > 0 else 0
        }
        
        # Ø¹Ø±Ø¶ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£Ø³Ø·ÙˆØ±ÙŠ
        self._display_legendary_report(all_results)
        
        return all_results
    
    async def _process_domain_legendary(self, domain: str) -> Dict:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Ø·Ø§Ù‚ Ø¨Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø£Ø³Ø·ÙˆØ±ÙŠØ©"""
        domain_results = {
            'urls': [],
            'parameters': Counter(),
            'source_breakdown': {}
        }
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ØµØ§Ø¯Ø± Ù…ØªØ¹Ø¯Ø¯Ø© - Ù…ÙŠØ²Ø© ØªÙ†Ø§ÙØ³ÙŠØ©
        tasks = []
        
        if not self.args.sources or 'wayback' in self.args.sources:
            tasks.append(self._fetch_from_source('wayback', domain))
        
        if not self.args.sources or 'sitemap' in self.args.sources:
            tasks.append(self._fetch_from_source('sitemap', domain))
        
        if not self.args.sources or 'javascript' in self.args.sources:
            tasks.append(self._fetch_from_source('javascript', domain))
        
        # ØªÙ†ÙÙŠØ° Ù…ØªÙˆØ§Ø²ÙŠ Ø¹Ø§Ù„ÙŠ Ø§Ù„Ø£Ø¯Ø§Ø¡
        if tasks:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    source_name = list(self.sources.keys())[i]
                    print_error(f"Ø®Ø·Ø£ ÙÙŠ Ù…ØµØ¯Ø± {source_name}: {result}")
                    continue
                
                if isinstance(result, list):
                    # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØªØµÙÙŠØ© Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
                    processed_urls = self._process_urls_intelligent(result, domain)
                    domain_results['urls'].extend(processed_urls)
                    
                    # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
                    for url_data in processed_urls:
                        for param in url_data.get('parameters', []):
                            domain_results['parameters'][param] += 1
        
        return domain_results
    
    async def _fetch_from_source(self, source_name: str, domain: str) -> List[str]:
        """Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ù…ØµØ¯Ø± Ù…Ø­Ø¯Ø¯"""
        try:
            source = self.sources[source_name]
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ù€ URLs
            max_urls = getattr(self.args, 'max_urls', 1000)
            
            # Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø¹Ø·ÙŠØ§Øª Ù…Ø¹ Ø§Ù„ØªØ­ÙƒÙ… Ø§Ù„Ø°ÙƒÙŠ Ø¨Ø§Ù„Ù…Ø¹Ø¯Ù„
            if hasattr(source, 'fetch_urls'):
                urls = await source.fetch_urls(domain)
            else:
                urls = []
            
            # ØªØ³Ø¬ÙŠÙ„ ÙØ¹Ø§Ù„ÙŠØ© Ø§Ù„Ù…ØµØ¯Ø±
            self.stats['source_effectiveness'][source_name] += len(urls)
            
            print_success(f"âœ“ {source_name}: {len(urls)} Ø±Ø§Ø¨Ø·")
            return urls[:max_urls] if urls else []
            
        except Exception as e:
            print_error(f"Ø®Ø·Ø£ ÙÙŠ Ù…ØµØ¯Ø± {source_name}: {e}")
            self.stats['processing_errors'].append(f"{source_name}: {e}")
            return []
    
    def _process_urls_intelligent(self, urls: List[str], domain: str) -> List[Dict]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø°ÙƒÙŠØ© Ù„Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ø¹ Ø§Ù„ÙÙ„ØªØ±Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
        processed_urls = []
        
        for url in urls:
            try:
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
                parameters = self.filter.extract_parameters(url)
                
                if not parameters:
                    continue
                
                # Ø§Ù„ÙÙ„ØªØ±Ø© Ø§Ù„Ø°ÙƒÙŠØ©
                if not self.args.no_filter:
                    filtered_params = self.filter.filter_parameters(parameters)
                else:
                    filtered_params = parameters
                
                if not filtered_params:
                    continue
                
                # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ù†Ø¸Ù
                cleaned_url = self.filter.create_cleaned_url(
                    url, filtered_params, 
                    getattr(self.args, 'placeholder', 'FUZZ')
                )
                
                # Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬
                url_data = {
                    'original_url': url,
                    'cleaned_url': cleaned_url,
                    'domain': domain,
                    'parameters': filtered_params,
                    'param_count': len(filtered_params)
                }
                
                processed_urls.append(url_data)
                
            except Exception as e:
                if getattr(self.args, 'debug', False):
                    print_error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø§Ø¨Ø· {url}: {e}")
                continue
        
        return processed_urls
    
    def _analyze_vulnerabilities(self, parameters: Counter) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù„Ù„Ø«ØºØ±Ø§Øª Ø§Ù„Ø£Ù…Ù†ÙŠØ© - Ù…ÙŠØ²Ø© Ø£Ø³Ø·ÙˆØ±ÙŠØ© ÙØ±ÙŠØ¯Ø©"""
        param_set = set(parameters.keys())
        
        # ÙƒØ´Ù Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø© Ù„Ù„Ø«ØºØ±Ø§Øª
        vulnerability_findings = detect_potential_vulnerability_params(param_set)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
        complexity_analysis = analyze_parameter_complexity(param_set)
        
        # ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©
        high_priority_params = []
        for param, score in complexity_analysis.items():
            if score >= 5:  # Ø¹ØªØ¨Ø© Ø¹Ø§Ù„ÙŠØ©
                high_priority_params.append({
                    'parameter': param,
                    'score': score,
                    'frequency': parameters[param]
                })
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø·
        high_priority_params.sort(key=lambda x: x['score'], reverse=True)
        
        return {
            'vulnerability_findings': vulnerability_findings,
            'complexity_analysis': complexity_analysis,
            'high_priority_params': high_priority_params[:20],  # Ø£ÙØ¶Ù„ 20
            'total_risk_params': len([p for p in complexity_analysis.values() if p >= 5])
        }
    
    def _display_legendary_report(self, results: Dict):
        """Ø¹Ø±Ø¶ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£Ø³Ø·ÙˆØ±ÙŠ"""
        print("")
        print_success("ğŸ† ØªÙ‚Ø±ÙŠØ± SpiderX Ø§Ù„Ø£Ø³Ø·ÙˆØ±ÙŠ")
        print("=" * 50)
        
        # Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        metrics = results['performance_metrics']
        print_info(f"â±ï¸  ÙˆÙ‚Øª Ø§Ù„ØªÙ†ÙÙŠØ°: {metrics['execution_time']:.2f} Ø«Ø§Ù†ÙŠØ©")
        print_info(f"ğŸŒ Ø§Ù„Ù†Ø·Ø§Ù‚Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {metrics['domains_processed']}")
        print_info(f"ğŸ”— Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·: {len(results['urls'])}")
        print_info(f"ğŸ¯ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù…ÙƒØªØ´ÙØ©: {metrics['total_parameters']}")
        print_info(f"âš¡ Ø§Ù„Ø³Ø±Ø¹Ø©: {metrics['urls_per_second']:.1f} Ø±Ø§Ø¨Ø·/Ø«Ø§Ù†ÙŠØ©")
        print("")
        
        # Ø£Ù‡Ù… Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
        if results['parameters']:
            print_info("ğŸ… Ø£Ù‡Ù… Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù…ÙƒØªØ´ÙØ©:")
            for param, count in results['parameters'].most_common(10):
                print(f"   {param}: {count} Ù…Ø±Ø©")
            print("")
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø«ØºØ±Ø§Øª Ø§Ù„Ø£Ù…Ù†ÙŠØ© - Ù…ÙŠØ²Ø© Ø­ØµØ±ÙŠØ©
        vuln_analysis = results.get('vulnerability_analysis', {})
        if vuln_analysis.get('vulnerability_findings'):
            print_warning("ğŸš¨ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù…Ø­ØªÙ…Ù„Ø© Ù„Ù„Ø«ØºØ±Ø§Øª Ø§Ù„Ø£Ù…Ù†ÙŠØ©:")
            for vuln_type, params in vuln_analysis['vulnerability_findings'].items():
                if params:
                    print(f"   {vuln_type}: {', '.join(params[:3])}")
            print("")
        
        # Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©
        high_priority = vuln_analysis.get('high_priority_params', [])
        if high_priority:
            print_info("ğŸ¯ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±:")
            for param_data in high_priority[:5]:
                print(f"   {param_data['parameter']} (Ø§Ù„Ù†Ù‚Ø§Ø·: {param_data['score']}, Ø§Ù„ØªÙƒØ±Ø§Ø±: {param_data['frequency']})")
            print("")
        
        # ÙØ¹Ø§Ù„ÙŠØ© Ø§Ù„Ù…ØµØ§Ø¯Ø±
        print_info("ğŸ“Š ÙØ¹Ø§Ù„ÙŠØ© Ø§Ù„Ù…ØµØ§Ø¯Ø±:")
        for source, count in self.stats['source_effectiveness'].items():
            percentage = (count / len(results['urls']) * 100) if results['urls'] else 0
            print(f"   {source}: {count} Ø±Ø§Ø¨Ø· ({percentage:.1f}%)")
        print("")
        
        print_success("ğŸ•·ï¸ SpiderX: Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ù…Ø³Ø­ Ø§Ù„Ø£Ø³Ø·ÙˆØ±ÙŠ Ø¨Ù†Ø¬Ø§Ø­!")
        
        # Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø²Ø§ÙŠØ§ Ø§Ù„ØªÙ†Ø§ÙØ³ÙŠØ©
        self._display_competitive_advantages()
    
    def _display_competitive_advantages(self):
        """Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø²Ø§ÙŠØ§ Ø§Ù„ØªÙ†Ø§ÙØ³ÙŠØ© Ø¹Ù„Ù‰ ParamSpider"""
        print("")
        print_success("ğŸ† Ù…Ø²Ø§ÙŠØ§ SpiderX Ø§Ù„ØªÙ†Ø§ÙØ³ÙŠØ© Ø¹Ù„Ù‰ ParamSpider:")
        print("=" * 55)
        
        advantages = [
            "âœ“ Ù…ØµØ§Ø¯Ø± Ù…ØªØ¹Ø¯Ø¯Ø©: Wayback + Sitemap + JavaScript (Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Wayback ÙÙ‚Ø·)",
            "âœ“ ÙÙ„ØªØ±Ø© Ø°ÙƒÙŠØ©: 60+ Ù…Ø¹Ø§Ù…Ù„ Ù…Ù…Ù„Ù„ + Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¨Ø·Ø§Ù‚Ø§Øª Ø§Ù„Ù†Ù…Ø·ÙŠØ©",
            "âœ“ ÙƒØ´Ù Ø§Ù„Ø«ØºØ±Ø§Øª: ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù„Ù„Ø«ØºØ±Ø§Øª Ø§Ù„Ø£Ù…Ù†ÙŠØ© ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙØ¹Ù„ÙŠ",
            "âœ“ Ø£Ø´ÙƒØ§Ù„ Ø§Ù„ØªØµØ¯ÙŠØ±: TXT + CSV + JSON Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©",
            "âœ“ Ø§Ù„Ø£Ø¯Ø§Ø¡: Ù…Ø¹Ø§Ù„Ø¬Ø© ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø®ÙŠÙˆØ·",
            "âœ“ Ø§Ù„Ø°ÙƒØ§Ø¡: ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª ÙˆØªÙˆØµÙŠØ§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±",
            "âœ“ Ø§Ù„ØªØ­ÙƒÙ… Ø¨Ø§Ù„Ù…Ø¹Ø¯Ù„: ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªÙƒÙŠÙÙŠ Ø§Ù„Ø°ÙƒÙŠ",
            "âœ“ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡: Ø¢Ù„ÙŠØ© Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ù‚ÙˆÙŠØ©",
            "âœ“ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª: ØªØ­Ù„ÙŠÙ„Ø§Øª Ø´Ø§Ù…Ù„Ø© ÙˆØªÙ‚Ø§Ø±ÙŠØ± Ù…Ø¨Ø§Ø´Ø±Ø©",
            "âœ“ Ø§Ù„ØªØ®ØµÙŠØµ: Ù‚Ø§Ø¨Ù„ÙŠØ© ØªÙƒÙˆÙŠÙ† Ø¹Ø§Ù„ÙŠØ© Ù…Ø¹ 20+ Ø®ÙŠØ§Ø± CLI"
        ]
        
        for advantage in advantages:
            print(f"  {advantage}")
        
        print("")
        print_success("ğŸ•·ï¸ SpiderX: Ø§Ù„Ø£Ø¯Ø§Ø© Ø§Ù„Ø£Ø³Ø·ÙˆØ±ÙŠØ© Ø§Ù„ÙˆØ­ÙŠØ¯Ø© Ù„Ø§ÙƒØªØ´Ø§Ù Ù…Ø¹Ø§Ù…Ù„Ø§Øª URL!")


def create_argument_parser():
    """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­Ù„Ù„ Ø§Ù„Ø­Ø¬Ø¬ Ù…Ø¹ Ø®ÙŠØ§Ø±Ø§Øª Ø´Ø§Ù…Ù„Ø©"""
    parser = argparse.ArgumentParser(
        description="SpiderX - Ø§Ù„Ø£Ø¯Ø§Ø© Ø§Ù„Ø£Ø³Ø·ÙˆØ±ÙŠØ© Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ø§Ù…Ù„Ø§Øª URL",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:
  python spiderx_legendary.py -d example.com
  python spiderx_legendary.py -l domains.txt --format json
  python spiderx_legendary.py -d example.com --sources wayback,sitemap
  python spiderx_legendary.py -d example.com --no-filter --stats
        """
    )
    
    # Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø£Ù‡Ø¯Ø§Ù
    target_group = parser.add_argument_group('Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø£Ù‡Ø¯Ø§Ù')
    target_group.add_argument('-d', '--domain', help='Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù Ù„Ù„Ù…Ø³Ø­')
    target_group.add_argument('-l', '--list', help='Ù…Ù„Ù ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù†Ø·Ø§Ù‚Ø§Øª')
    
    # Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ù…ØµØ§Ø¯Ø±
    source_group = parser.add_argument_group('Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ù…ØµØ§Ø¯Ø±')
    source_group.add_argument('--sources', help='Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ØµØ§Ø¯Ø±: wayback,sitemap,javascript,all (Ø§ÙØªØ±Ø§Ø¶ÙŠ: all)')
    
    # Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
    output_group = parser.add_argument_group('Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬')
    output_group.add_argument('-o', '--output', help='Ø§Ø³Ù… Ù…Ù„Ù Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬')
    output_group.add_argument('--format', choices=['txt', 'csv', 'json'], default='txt', help='Ø´ÙƒÙ„ Ø§Ù„ØªØµØ¯ÙŠØ±')
    output_group.add_argument('--no-save', action='store_true', help='Ø¹Ø¯Ù… Ø§Ù„Ø­ÙØ¸ØŒ Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙ‚Ø·')
    
    # Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„ÙÙ„ØªØ±Ø©
    filter_group = parser.add_argument_group('Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„ÙÙ„ØªØ±Ø©')
    filter_group.add_argument('--boring-list', help='Ù…Ù„Ù Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù…Ù…Ù„Ø© Ø§Ù„Ù…Ø®ØµØµØ©')
    filter_group.add_argument('--no-filter', action='store_true', help='ØªØ¹Ø·ÙŠÙ„ ÙÙ„ØªØ±Ø© Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª')
    filter_group.add_argument('--custom-filter', nargs='+', help='Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù…Ø®ØµØµØ© Ù„Ù„ÙÙ„ØªØ±Ø©')
    
    # Ø®ÙŠØ§Ø±Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©
    advanced_group = parser.add_argument_group('Ø®ÙŠØ§Ø±Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©')
    advanced_group.add_argument('--placeholder', default='FUZZ', help='Ù†Øµ Ø¨Ø¯ÙŠÙ„ Ù„Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª')
    advanced_group.add_argument('--proxy', help='Ø¨Ø±ÙˆÙƒØ³ÙŠ HTTP')
    advanced_group.add_argument('--timeout', type=int, default=30, help='Ù…Ù‡Ù„Ø© Ø§Ù„Ø·Ù„Ø¨ Ø¨Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ')
    advanced_group.add_argument('--max-urls', type=int, default=10000, help='Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø±ÙˆØ§Ø¨Ø· Ù„ÙƒÙ„ Ù†Ø·Ø§Ù‚')
    
    # Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„
    analysis_group = parser.add_argument_group('Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„')
    analysis_group.add_argument('--stats', action='store_true', help='Ø¹Ø±Ø¶ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù…ÙØµÙ„Ø©')
    analysis_group.add_argument('--vuln-analysis', action='store_true', help='ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù„Ù„Ø«ØºØ±Ø§Øª Ø§Ù„Ø£Ù…Ù†ÙŠØ©')
    
    # Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„ØªØµØ­ÙŠØ­
    debug_group = parser.add_argument_group('Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„ØªØµØ­ÙŠØ­')
    debug_group.add_argument('-v', '--verbose', action='store_true', help='Ø¥Ø®Ø±Ø§Ø¬ Ù…ÙØµÙ„')
    debug_group.add_argument('--debug', action='store_true', help='ØªÙ…ÙƒÙŠÙ† ÙˆØ¶Ø¹ Ø§Ù„ØªØµØ­ÙŠØ­')
    
    return parser


async def main():
    """Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    parser = create_argument_parser()
    args = parser.parse_args()
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø¬Ø¬
    if not args.domain and not args.list:
        parser.error("ÙŠØ±Ø¬Ù‰ ØªÙˆÙÙŠØ± Ø®ÙŠØ§Ø± -d Ø£Ùˆ -l")
    
    if args.domain and args.list:
        parser.error("ÙŠØ±Ø¬Ù‰ ØªÙˆÙÙŠØ± Ø®ÙŠØ§Ø± -d Ø£Ùˆ -lØŒ ÙˆÙ„ÙŠØ³ ÙƒÙ„Ø§Ù‡Ù…Ø§")
    
    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†Ø·Ø§Ù‚Ø§Øª
    domains = []
    if args.domain:
        domains = [args.domain.strip().lower().replace('https://', '').replace('http://', '')]
    elif args.list:
        try:
            with open(args.list, 'r') as f:
                domains = [
                    line.strip().lower().replace('https://', '').replace('http://', '')
                    for line in f.readlines()
                    if line.strip() and not line.startswith('#')
                ]
                domains = list(set(domains))  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…ÙƒØ±Ø±Ø§Øª
        except FileNotFoundError:
            print_error(f"Ù…Ù„Ù Ø§Ù„Ù†Ø·Ø§Ù‚Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {args.list}")
            sys.exit(1)
        except Exception as e:
            print_error(f"Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù Ø§Ù„Ù†Ø·Ø§Ù‚Ø§Øª: {e}")
            sys.exit(1)
    
    if not domains:
        print_error("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†Ø·Ø§Ù‚Ø§Øª ØµØ§Ù„Ø­Ø© Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©")
        sys.exit(1)
    
    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ù…ØµØ§Ø¯Ø±
    if args.sources:
        if args.sources.lower() == 'all':
            args.sources = ['wayback', 'sitemap', 'javascript']
        else:
            args.sources = [s.strip() for s in args.sources.split(',')]
    
    # Ø¥Ù†Ø´Ø§Ø¡ ÙƒØ§Ø¦Ù† SpiderX Ø§Ù„Ø£Ø³Ø·ÙˆØ±ÙŠ
    spiderx = LegendarySpiderX(args)
    
    try:
        # ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ø³Ø­ Ø§Ù„Ø£Ø³Ø·ÙˆØ±ÙŠ
        results = await spiderx.legendary_scan(domains)
        
        # ØªØµØ¯ÙŠØ± Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… ØªØ¹Ø·ÙŠÙ„Ù‡
        if not args.no_save and results['urls']:
            exporter = ResultExporter(args)
            output_file = exporter.export(results)
            if output_file:
                print_success(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ: {output_file}")
        
        print_success("âœ¨ Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ù…Ø³Ø­ Ø§Ù„Ø£Ø³Ø·ÙˆØ±ÙŠ Ø¨Ù€ SpiderX!")
        
    except KeyboardInterrupt:
        print_error("ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…")
        sys.exit(1)
    except Exception as e:
        print_error(f"Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {e}")
        if args.debug:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print_error("ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¹Ù…Ù„ÙŠØ©")
        sys.exit(1)